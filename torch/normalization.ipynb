{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a30d0d",
   "metadata": {},
   "source": [
    "# Batch Normalizationとは"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02faaf80",
   "metadata": {},
   "source": [
    "画像なら 2d\n",
    "\n",
    "全結合層なら 1d\n",
    "\n",
    "動画なら 3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e4c14a",
   "metadata": {},
   "source": [
    "バッチごとに正規化を行う。\n",
    "平均で引いて標準偏差で割る。これを行うことで、共変量シフトを防げる。\n",
    "共変量シフトとは、同じデータセットから抽出したデータの分布が異なること。訓練データで最適化されたモデルは、テストで精度を欠いてしまう。ニューラルネットワークでは、初期のデータが何度も層を通ることで変化し、学習に影響をおよぼす。これを内部の共変量シフトという。  \n",
    "これまで、データセットの正規化（白色化？）を行うことでこれを防いできた（これかどうかは分からない）が、ネットワーク内部では異なってしまう。これを解決するために層への入力データを正規化する。これをバッチ正規化という。\n",
    "- 大きな学習係数を使える  \n",
    "今まではパラメータにより勾配消失や爆発が起こっていたが、各層で正規化することでこれを防ぐ。学習の収束速度が向上する。  \n",
    "- 正則化効果あり  \n",
    "dropout不要？\n",
    "- 初期値にそれほど依存しない？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
